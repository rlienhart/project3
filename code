---
title: "R Project 3"
author: "Sean Finnigan, Ryan Lienhart, Rachel McConaghy"
date: "5/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```
#### 3. (20pts) Simulate the data (n=1000) from a lognormal distribution with the parameters of your choice. Perform the following tasks:

```{r}
# Simulate Data
set.seed(123)
data <- rlnorm(1000, meanlog = 0, sdlog = 1)
```

##### a) Plot the histogram of your data in R

```{r}
hist(data, freq=FALSE, breaks=30)
```

##### b) Estimate the LN-parameters using the Method of Moments. Show your derivations.

$$\mu_1'=\mu=E(Y)=e^{\mu+\frac{\sigma^2}{2}}$$

$$m_1'=\frac{1}{n}\sum_{i=0}^ny_i=\bar{Y}$$

$$\Rightarrow{e}^{\mu+\frac{\sigma^2}{2}}=\bar{Y}$$

$$\begin{aligned}
\mu_2'&=E(Y^2)=V(Y)+E(Y)^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{\mu+\frac{\sigma^2}{2}})^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}\bar{Y}^2
\end{aligned}$$

$$m_2'=\frac{1}{n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{e}^{\sigma^2}\bar{Y}^2=\frac{1}{n}\sum_{i=0}^ny_i^2$$

Solve for $\hat{\mu}_{MM}$ and $\hat{\sigma}^2_{MM}$:

$$e^{\sigma^2}=\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{\hat{\sigma}^2_{MM}}=\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)$$

$$\mu+\frac{\sigma^2}{2}=\ln(\bar{Y})$$

$$\begin{aligned}
\Rightarrow\hat{\mu}_{MM}&=\ln(\bar{Y})-\frac{\sigma^2}{2}\\
&=\ln(\bar{Y})-\frac{1}{2}\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)\\
&=\ln(\bar{Y})+\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)^{-\frac{1}{2}}\\
&=\ln(\bar{Y})+\ln\left(\bar{Y}\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)\\
&=\ln\left(\bar{Y}^2\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)
\end{aligned}$$

```{r}
n <- length(data)

mu.mm <- log(((mean(data)^2))*sqrt(n/sum(data^2)))
mu.mm

s2.mm <- log((1/(mean(data)^2))*(1/n)*sum(data^2))
s2.mm
```

##### c) Estimate the LN-parameters using the Maximum Likelihood Estimation. Show your derivations.

$$\begin{aligned}
L(\mu,\sigma^2)&=f(y_1, y_2,...,y_n|\mu,\sigma^2)\\
&=\prod_{i=1}^n\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}\\
&=\left(\prod_{i=1}^n\frac{1}{y_i}\right)\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}}e^{-\frac{\sum_{i=1}^n(\ln(y_i)-\mu)^2}{2\sigma^2}}
\end{aligned}$$
$$\log(L(\mu,\sigma^2))=-\log(\prod_{i=1}^ny_i)-\frac{n}{2}\log(\sigma^2)-\frac{n}{2}\log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2$$
$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\mu}=\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)=0$$
$$\begin{aligned}
&\Rightarrow\sum_{i=1}^n\ln(y_i)-n\mu=0\\
&\Rightarrow{n}\mu=\sum_{i=1}^n\ln(y_i)\\
&\Rightarrow\hat{\mu}_{MLE}=\frac{1}{n}\sum_{i=1}^n\ln(y_i)
\end{aligned}$$

$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\sigma^2}=-\frac{n}{\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0$$
$$\begin{aligned}
&\Rightarrow{-n}+\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0\\
&\Rightarrow\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=n\\
&\Rightarrow\hat{\sigma}^2_{MLE}=\frac{1}{n}\sum_{i=1}^n(\ln(y_i)-\mu)^2\\
\end{aligned}$$

```{r}
mu.mle <- (1/n)*sum(log(data))
mu.mle

s2.mle <- (1/n)*sum((log(data)-mu.mle)^2)
s2.mle
```

##### d) Plot the density using estimates in b) repeat the same using estimated in c). Overlay these two densities on your original histogram in a). Discuss your findings.

```{r}
hist(data, freq=FALSE, breaks = 30, ylim=c(0, 0.7))
a <- seq(0, 25, by = 0.01)
b <- dlnorm(a, mu.mm, s2.mm)
points(a, b, col="red")

hist(data, freq=FALSE, breaks = 30, ylim=c(0, 0.7))
x <- seq(0, 25, by = 0.01)
y <- dlnorm(x, mu.mle, s2.mle)
points(x, y, col="red")
```

The shape of the plotted density of a lognormal distribution using the method of moments estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with mean 0 and variance 1. The shape of the plotted density of a lognormal distribution using the maximum likelihood estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with mean 0 and variance 1. From these graphs we cannot see an obvious difference in the performance of the estimators, so further comparisons of the estimators will be used to determine if the method of moments estimators or maximum likelihood estimators perform better.

##### e) Calculate the Bias and MSE for both estimators computed in b) and c). Present your summary of the results in a table format. Discuss your findings.

```{r}
bias.mu.mm <- mu.mm - mean(data)
bias.mu.mle <- mu.mle - mean(data)
bias.s2.mm <- s2.mm - sd(data)^2
bias.s2.mle <- s2.mle - sd(data)^2

#mse.mu.mm <- + bias.mu.mm^2
#mse.mu.mle <- + bias.mu.mle^2
#mse.s2.mm <- + bias.s2.mm^2
#mse.s2.mle <- + bias.s2.mle^2

table <-  data.frame(bias.mu=c(bias.mu.mm, bias.mu.mle), bias.s2=c(bias.s2.mm, bias.s2.mle), mse.mu=c(0,0), mse.s2=c(0,0))
colnames(table) = c("Bias of $\\mu$", "Bias of $\\sigma^2$", "MSE of $\\mu$", "MSE of $\\sigma^2$")
rownames(table) = c("Method of Moments", "Maximum Likelihood Estimation")
kable(table)
```

##### f) Calculate the efficiency of the estimator in c) relative to that in b) for both parameters of interest. Discuss your findings.

```{r}

```

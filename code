---
title: "R Project 3"
author: "Sean Finnigan, Ryan Lienhart, Rachel McConaghy"
date: "5/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
```

#### 1. (20pts) Consider a left truncated Gaussian distribution with parameters ùúá and ùúé2.  The pdf of this distribution will be modified to include the truncation point. Refer to your lecture notes when this was discussed in class.  
#####a) Write down the density function of the left-truncated random variable if the truncation point is b.
#####b) Derive the CDF of the left-truncated random variable. You will need to use some knowledge from STA401.
#####c) Develop a Likelihood function for the left-truncated distribution.
#####d) Develop the log-likelihood function for the left-truncated distribution.
#####e) How are you going to find MLEs? What options do you have? Discuss and solve this problem.
In order to find the MLEs, we could try taking the derivative with respect to each variable $\mu$, $\sigma$, and b, and setting it equal to zero in order to solve for each parameter.
#####f) Perform computations in R: generate a sample from the left-truncated Gaussian distribution and calculate the MLEs by fitting the appropriate model and using the method suggested in e).  
#####g) Plot the sample and the density function in R. Label your plot accordingly.

####2. (20pts) Repeat Q1 using Lognormal distribution. Consider a left truncated Lognormal distribution with parameters ùúá and ùúé2.  The pdf of this distribution will be modified to include the truncation point. Refer to your lecture notes when this was discussed in class.  
#####h) Write down the density function of the left-truncated random variable if the truncation point is b.
$$\frac{f(y_i|\mu,\sigma^2)}{1-F(b|\mu, \sigma^2)}=\frac{\frac{1}{y_i\sigma\sqrt{2\pi}}e^-{\frac{(ln(y_i)-\mu)^2}{2\sigma^2}}}{1-\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt{2}}))}$$
#####i) Derive the CDF of the left-truncated random variable. You will need to use some knowledge from STA401.
$$\int_{0}^{\infty} \frac{\frac{1}{y_i\sigma\sqrt{2\pi}}e^-{\frac{(ln(y_i)-\mu)^2}{2\sigma^2}}}{1-\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt{2}}))} \; dy_i$$
$$= \frac{1}{1-\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt{2}}))} \int_{0}^{\infty} \frac{1}{y_i\sigma\sqrt{2\pi}}e^-{\frac{-(ln(y_i)-\mu)^2}{2\sigma^2}}$$
$$= \frac{\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt2})}{1-\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt2})}$$
#####j) Develop a Likelihood function for the left-truncated distribution.
$$L(\mu, \sigma^2) = \prod_{i = 1}^{n} \frac{\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt2})}{1-\frac{1}{2}(1+erf(\frac{ln(b)-\mu}{\sigma\sqrt2})}$$
$$= \frac{(\prod_{i = 1}^{n}\frac{1}{y_i})(\frac{1}{\sigma^22\pi})^\frac{n}{2}e^\frac{-\sum_{i = 1}^{n} ln(y_i)-\mu)^2}{2\sigma^2}}{1-\frac{1}{2}(1 + erf(\frac{ln(b)-\mu}{\sigma\sqrt{2}}))^n}$$
#####k) Develop the log-likelihood function for the left-truncated distribution.
$$log(L(\mu,\sigma^2)) = -log(\prod_{i = 1}^{n} y_i) - \frac{n}{2}log(\sigma^2) - \frac{n}{2}log(2\pi) - \frac{1}{2\sigma^2}\sum_{i = 1}^{n} (ln(y_i)-\mu)^2 - nlog(1-\frac{1}{2}(1 + erf(\frac{ln(b)-\mu}{\sigma\sqrt2})))$$
#####l) How are you going to find MLEs? What options do you have? Discuss and solve this problem.
#####m) Perform computations in R: generate a sample from the left-truncated Lognormal distribution and calculate the MLEs by fitting the appropriate model and using the method suggested in e).  
#####n) Plot the sample and the density function in R. Label your plot accordingly. 

#### 3. (20pts) Simulate the data (n=1000) from a lognormal distribution with the parameters of your choice. Perform the following tasks:

```{r}
set.seed(123)
n <- 1000
mu <- 0
s2 <- 1
x <- rlnorm(n, meanlog = mu, sdlog = s2)
```

##### a) Plot the histogram of your data in R

```{r}
hist(x, freq=FALSE, breaks=30)
```

##### b) Estimate the LN-parameters using the Method of Moments. Show your derivations.

$$\mu_1'=\mu=E(Y)=e^{\mu+\frac{\sigma^2}{2}}$$

$$m_1'=\frac{1}{n}\sum_{i=0}^ny_i=\bar{Y}$$

$$\Rightarrow{e}^{\mu+\frac{\sigma^2}{2}}=\bar{Y}$$

$$\begin{aligned}
\mu_2'&=E(Y^2)=V(Y)+E(Y)^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{\mu+\frac{\sigma^2}{2}})^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}\bar{Y}^2
\end{aligned}$$

$$m_2'=\frac{1}{n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{e}^{\sigma^2}\bar{Y}^2=\frac{1}{n}\sum_{i=0}^ny_i^2$$

Solve for $\hat{\mu}_{MM}$ and $\hat{\sigma}^2_{MM}$:

$$e^{\sigma^2}=\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{\hat{\sigma}^2_{MM}}=\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)$$

$$\mu+\frac{\sigma^2}{2}=\ln(\bar{Y})$$

$$\begin{aligned}
\Rightarrow\hat{\mu}_{MM}&=\ln(\bar{Y})-\frac{\sigma^2}{2}\\
&=\ln(\bar{Y})-\frac{1}{2}\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)\\
&=\ln(\bar{Y})+\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)^{-\frac{1}{2}}\\
&=\ln(\bar{Y})+\ln\left(\bar{Y}\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)\\
&=\ln\left(\bar{Y}^2\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)
\end{aligned}$$

```{r}
mmests = matrix(NA, 20000, 2)
for (i in 1:20000) {
  y <- rlnorm(1000, meanlog = 0, sdlog = 1)
  mu.mm <- log(((mean(y)^2))*sqrt(1000/sum(y^2)))
  s2.mm <- log((1/(mean(y)^2))*(1/1000)*sum(y^2))
  mmests[i,1] = mu.mm
  mmests[i,2] = s2.mm
}
mu.s2.mm <- colMeans(mmests)
mu.s2.mm
```

##### c) Estimate the LN-parameters using the Maximum Likelihood Estimation. Show your derivations.

$$\begin{aligned}
L(\mu,\sigma^2)&=f(y_1, y_2,...,y_n|\mu,\sigma^2)\\
&=\prod_{i=1}^n\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}\\
&=\left(\prod_{i=1}^n\frac{1}{y_i}\right)\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}}e^{-\frac{\sum_{i=1}^n(\ln(y_i)-\mu)^2}{2\sigma^2}}
\end{aligned}$$
$$\log(L(\mu,\sigma^2))=-\log(\prod_{i=1}^ny_i)-\frac{n}{2}\log(\sigma^2)-\frac{n}{2}\log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2$$
$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\mu}=\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)=0$$
$$\begin{aligned}
&\Rightarrow\sum_{i=1}^n\ln(y_i)-n\mu=0\\
&\Rightarrow{n}\mu=\sum_{i=1}^n\ln(y_i)\\
&\Rightarrow\hat{\mu}_{MLE}=\frac{1}{n}\sum_{i=1}^n\ln(y_i)
\end{aligned}$$

$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\sigma^2}=-\frac{n}{\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0$$
$$\begin{aligned}
&\Rightarrow{-n}+\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0\\
&\Rightarrow\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=n\\
&\Rightarrow\hat{\sigma}^2_{MLE}=\frac{1}{n}\sum_{i=1}^n(\ln(y_i)-\mu)^2\\
\end{aligned}$$

```{r}
mlests = matrix(NA, 20000, 2)
for (i in 1:20000) {
  y <- rlnorm(1000, meanlog = 0, sdlog = 1)
  mu.mle <- (1/n)*sum(log(y))
  s2.mle <- (1/n)*sum((log(y)-mu.mle)^2)
  mlests[i,1] = mu.mle
  mlests[i,2] = s2.mle
}
mu.s2.mle <- colMeans(mlests)
mu.s2.mle
```

##### d) Plot the density using estimates in b) repeat the same using estimated in c). Overlay these two densities on your original histogram in a). Discuss your findings.

```{r}
hist(x, freq=FALSE, breaks = 30, ylim=c(0, 0.7))
a <- seq(0, 25, by = 0.01)
b <- dlnorm(a, mu.s2.mm[1], mu.s2.mm[2])
points(a, b, col="red")

hist(x, freq=FALSE, breaks = 30, ylim=c(0, 0.7))
c <- dlnorm(a, mu.s2.mle[1], mu.s2.mle[2])
points(a, c, col="red")
```

The shape of the plotted density of a lognormal distribution using the method of moments estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with mean 0 and variance 1. The shape of the plotted density of a lognormal distribution using the maximum likelihood estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with mean 0 and variance 1. From these graphs we cannot see an obvious difference in the performance of the estimators, so further comparisons of the estimators will be used to determine if the method of moments estimators or maximum likelihood estimators perform better.

##### e) Calculate the Bias and MSE for both estimators computed in b) and c). Present your summary of the results in a table format. Discuss your findings.

```{r}
bias.mu.mm <- mu.s2.mm[1] - mu
bias.mu.mle <- mu.s2.mle[1] - mu
bias.s2.mm <- mu.s2.mm[2] - s2
bias.s2.mle <- mu.s2.mle[2] - s2

mse.mu.mm <- sd(mmests[,1])^2 + bias.mu.mm^2
mse.mu.mle <- sd(mlests[,1])^2 + bias.mu.mle^2
mse.s2.mm <- sd(mmests[,2])^2 + bias.s2.mm^2
mse.s2.mle <- sd(mmests[,2])^2 + bias.s2.mle^2

table <-  data.frame(bias.mu=c(bias.mu.mm, bias.mu.mle), bias.s2=c(bias.s2.mm, bias.s2.mle), mse.mu=c(mse.mu.mm, mse.mu.mle), mse.s2=c(mse.s2.mm, mse.s2.mle))
colnames(table) = c("Bias of $\\mu$", "Bias of $\\sigma^2$", "MSE of $\\mu$", "MSE of $\\sigma^2$")
rownames(table) = c("Method of Moments", "Maximum Likelihood Estimation")
kable(table)
```

##### f) Calculate the efficiency of the estimator in c) relative to that in b) for both parameters of interest. Discuss your findings.

```{r}
eff.mu <- (sd(mmests[,1])^2)/(sd(mlests[,1])^2)
eff.mu
eff.s2 <- (sd(mmests[,2])^2)/(sd(mlests[,2])^2)
eff.s2
```

***

####4. (20pts) Suppose you want to test the following hypotheses: 
Ho: Œº = 30,000
Ha: Œº  ‚â† 30,000
Use Œ±=0.05 and Œ±=0.01.
#####a) Define a rejection region for a given Œ±-level. Assume œÉ=5,000. All derivations must be included in the report. [Note: the sample size is small!]
Although the sample size is small, because we know the population standard deviation, we can use standard normal.
```{r}
30000 + (5000/sqrt(29))*qnorm(.025)
30000 + (5000/sqrt(29))*qnorm(.975)

30000 + (5000/sqrt(29))*qnorm(.005)
30000 + (5000/sqrt(29))*qnorm(.995)
```

Given a small sample where n = 29:

For Œ±=0.05, RR = {T < 28180.22 or T > 31819.78}

For Œ±=0.01, RR = {T < 27608.4 or T > 32391.6}


#####b) Construct two Power Curves for various values of Œ≤- Type II error rate. These curves must be on the same plot. [Note: you will need to consider various values for the sample mean.] 

```{r}
powerfunction <- function(mu, sd, alpha1, alpha2, n) {
  vec <- seq(20001,39999,1)
  se <- (sd/sqrt(n))
  
  q1 = qnorm(1-alpha1/2)
  q2 = qnorm(1-alpha2/2)
  
  rr_lower1 <- ((mu-q1*se)-vec)/se
  rr_upper1 <- ((mu+q1*se)-vec)/se
  
  rr_lower2 <- ((mu-q2*se)-vec)/se
  rr_upper2 <- ((mu+q2*se)-vec)/se
  
  power1 <- pnorm(rr_upper1, lower.tail = FALSE) + 
    pnorm(rr_lower1, lower.tail=TRUE)
  power2 <- pnorm(rr_upper2, lower.tail = FALSE) +
    pnorm(rr_lower2, lower.tail=TRUE)
  
  
  ggplot() +
  geom_path(aes(x=vec, y=power1), size=0.8, color="royalblue") +
  geom_path(aes(x=vec, y=power2), size=0.8, color="limegreen") +
  labs(y="Power", x="mu")
  
}

powerfunction(30000,5000,.05,.01,29)
```

#####c) Reflect on your findings. How does Œ±-level impact the shape of the power curve? How does power changes with Œ±? How does power changes with Œ≤? A minimum 5 full sentences are required for full credit. 

Increasing Œ± causes the shape of the power curve to becomes more narrow. The area under the curve also increases as Œ± does. This is a result of power increasing as Œ± increases. As the value of beta decreases, the power increases. This happens because the distance between the specified null value and the specified alternative value increases.

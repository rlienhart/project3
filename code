---
title: "R Project 3"
author: "Sean Finnigan, Ryan Lienhart, Rachel McConaghy"
date: "5/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
```

#### 1. (20pts) Consider a left truncated Gaussian distribution with parameters $\mu$ and $\sigma^2$. The pdf of this distribution will be modified to include the truncation point. Refer to your lecture notes when this was discussed in class.

##### a) Write down the density function of the left-truncated random variable if the truncation point is b.

The density function of the left-truncated Gaussian distribution is:

$$f(x;\mu,\sigma^2,b) =\frac{g(x|\mu,\sigma^2)}{1-G(b|\mu, \sigma^2)}= \frac{\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{b-\mu}{\sigma})}$$

$g(x|\mu,\sigma^2)$ is the PDF of the Gaussian distribution and $G(b|\mu, \sigma^2)$ is the CDF Gaussian distribution at b.

##### b) Derive the CDF of the left-truncated random variable. You will need to use some knowledge from STA401.

Take the integral from 0 to $\infty$ of the PDF of the left-truncated Gaussian distribution to find the CDF:

$$F(x;\mu,\sigma^2,b) = \int_{0}^{\infty} \frac{\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{b-\mu}{\sigma})} \; dx$$

$$= \frac{1}{1-\Phi(\frac{b-\mu}{\sigma})} \int_{0}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \; dx$$

$$=\frac{\Phi(\frac{x-\mu}{\sigma})}{1-\Phi(\frac{b-\mu}{\sigma})}$$

##### c) Develop a Likelihood function for the left-truncated distribution.

Take the product from 1 to n of the left-truncated Gaussian PDF to find the likelihood function:

$$L(\mu, \sigma^2) = \prod_{i = 1}^{n} \frac{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{b-\mu}{\sigma})}$$

$$= \frac{(\frac{1}{\sigma^22\pi})^\frac{n}{2}e^{-\frac{\sum_{i = 1}^{n} (x_i-\mu)^2}{2\sigma^2}}}{(1-\Phi(\frac{b-\mu}{\sigma}))^n}$$

##### d) Develop the log-likelihood function for the left-truncated distribution.

Take the logarithm of the likelihood function above to find the log-likelihood function:

$$\log(L(\mu,\sigma^2))=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-n\log(1-\Phi(\frac{b-\mu}{\sigma}))-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_{i}-\mu)^2$$

Note: $\log(L(\mu,\sigma^2))=\log\left(\prod_{i = 1}^{n}\frac{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}{1-\phi(\frac{b-\mu}{\sigma})}\right)=\sum_{i = 1}^{n}\log\left(\frac{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}{1-\phi(\frac{b-\mu}{\sigma})}\right)$ is used in part f.

##### e) How are you going to find MLEs? What options do you have? Discuss and solve this problem.

Normally to derive the MLEs, we would take the derivative of the log-likelihood function in terms of our estimators. That being said, our log-likelihood function is very complex and hard to get a derivative from. Our best option would be to use the optim function in r to find the MLEs for the left truncated Gaussian distribution.

##### f) Perform computations in R: generate a sample from the left-truncated Gaussian distribution and calculate the MLEs by fitting the appropriate model and using the method suggested in e).

```{r}
set.seed(100)
x <- rnorm(1000,4,2)
x <- x[x>2]

LL <- function(pars, y) {
  sum <- 0
  mu <- pars[1]
  s2 <- pars[2]
  for (i in 1:length(y)) {
    sum <- sum + log((dnorm(y[i], mean=mu, sd=sqrt(s2)))/(1-pnorm(2, mean=mu, sd=sqrt(s2))))
  }
  return(-sum)
}

pars <- c(4,4)
mle <- optim(par=pars, fn=LL, method="Nelder-Mead", y=x)$par
mle
```

$\hat{\mu}_{MLE}=4.1185$

$\hat{\sigma^2}_{MLE}=3.9897$

#####  g) Plot the sample and the density function in R. Label your plot accordingly.

```{r}
hist(x, prob=TRUE, main="Density Using MLE Estimates Over Left Truncated Gaussian Data", xlab="Left Truncated Gaussian Data")
curve((dnorm(x, mean=mle[1], sd=sqrt(mle[2])))/(1-pnorm(2, mean=mle[1], sd=sqrt(mle[2]))), col="red", lwd=2, add=TRUE, yaxt="n")
```

We see in the graph above that the MLE gained from using the optim function very accurately reflects the randomly generated truncated gaussian data.

***

#### 2. (20pts) Repeat Q1 using Lognormal distribution. Consider a left truncated Lognormal distribution with parameters $\mu$ and $\sigma^2$. The pdf of this distribution will be modified to include the truncation point. Refer to your lecture notes when this was discussed in class.

##### h) Write down the density function of the left-truncated random variable if the truncation point is b.

The density function of the left-truncated lognormal distribution is:

$$f(y|\mu,\sigma^2,b) =\frac{g(y|\mu,\sigma^2)}{1-G(b|\mu, \sigma^2)}=\frac{\frac{1}{y\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y)-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{\ln(b)-\mu}{\sigma})}$$

$g(y|\mu,\sigma^2)$ is the PDF of the lognormal distribution and $G(b|\mu, \sigma^2)$ is the CDF lognormal distribution at b.

##### i) Derive the CDF of the left-truncated random variable. You will need to use some knowledge from STA401.

Take the integral from 0 to $\infty$ of the PDF of the left-truncated lognormal distribution to find the CDF:

$$F(y|\mu,\sigma^2,b)=\int_{0}^{\infty} \frac{\frac{1}{y\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y)-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{\ln(b)-\mu}{\sigma})} \; dy$$

$$= \frac{1}{1-\Phi(\frac{\ln(b)-\mu}{\sigma})} \int_{0}^{\infty} \frac{1}{y\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y)-\mu)^2}{2\sigma^2}} \; dy$$

$$= \frac{\Phi(\frac{\ln(x)-\mu}{\sigma})}{1-\Phi(\frac{\ln(b)-\mu}{\sigma})}$$

##### j) Develop a Likelihood function for the left-truncated distribution.

Take the product from 1 to n of the left-truncated lognormal PDF to find the likelihood function:

$$L(\mu, \sigma^2) = \prod_{i = 1}^{n} \frac{\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}}{1-\Phi(\frac{\ln(b)-\mu}{\sigma})}$$

$$= \frac{(\prod_{i = 1}^{n}\frac{1}{y_i})(\frac{1}{\sigma^22\pi})^\frac{n}{2}e^{-\frac{\sum_{i = 1}^{n} (\ln(y_i)-\mu)^2}{2\sigma^2}}}{(1-\Phi(\frac{\ln(b)-\mu}{\sigma}))^n}$$

##### k) Develop the log-likelihood function for the left-truncated distribution.

Take the logarithm of the likelihood function above to find the log-likelihood function:

$$\log(L(\mu,\sigma^2)) = -\log(\prod_{i = 1}^{n} y_i) - \frac{n}{2}\log(\sigma^2) - \frac{n}{2}\log(2\pi) - \frac{1}{2\sigma^2}\sum_{i = 1}^{n} (\ln(y_i)-\mu)^2 - n\log(1-\phi(\frac{\ln(b)-\mu}{\sigma}))$$

Note: $\log(L(\mu,\sigma^2))=\log\left(\prod_{i = 1}^{n} \frac{\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}}{1-\phi(\frac{\ln(b)-\mu}{\sigma})}\right)=\sum_{i = 1}^{n}\log\left(\frac{\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}}{1-\phi(\frac{\ln(b)-\mu}{\sigma})}\right)$ is used in part m.

##### l) How are you going to find MLEs? What options do you have? Discuss and solve this problem.

Normally to derive the MLEs, we would take the derivative of the log-likelihood function in terms of our estimators. That being said, our log-likelihood function is very complex and hard to get a derivative from. Our best option would be to use the optim function in r to find the MLEs for the left truncated lognormal distribution.

##### m) Perform computations in R: generate a sample from the left-truncated Lognormal distribution and calculate the MLEs by fitting the appropriate model and using the method suggested in l).

```{r}
set.seed(100)
# generate sample of size 5000 from left truncated lognormal distribution with truncation point b=10
ltruncln <- rep(NA, 5000)
for (i in 1:5000) {
  while(is.na(ltruncln[i])) {
    y <- rlnorm(1, meanlog=0, sdlog=sqrt(1))
    if (y >= 10) {
      ltruncln[i] <- y
    }
  }
}

LL.ln <- function(pars, y) {
  sum <- 0
  mu <- pars[1]
  s2 <- pars[2]
  for (i in 1:length(y)) {
    sum <- sum + (log((dlnorm(y[i], meanlog=mu, sdlog=sqrt(s2)))/(1-plnorm(10, meanlog=mu, sdlog=sqrt(s2)))))
  }
  return(-sum)
}

pars <- c(0,1)
ln.mle <- optim(par=pars, fn=LL.ln, method="Nelder-Mead", y=ltruncln)$par
ln.mle
```

$\hat{\mu}_{MLE}=-0.4357126$

$\hat{\sigma}^2_{MLE}=1.1285176$

##### n) Plot the sample and the density function in R. Label your plot accordingly.

```{r}
hist(ltruncln, prob=TRUE, breaks=30, main="Density Using MLE Estimates Over Left Truncated Lognormal Data", xlab="Value of Left-Truncated Lognormal Random Variable")
curve(dlnorm(x, meanlog=ln.mle[1], sdlog=sqrt(ln.mle[2]))/(1-plnorm(10, meanlog=ln.mle[1], sdlog=sqrt(ln.mle[2]))), col="red", lwd=2, add=TRUE, yaxt="n")
```

The shape of the plotted density of a left truncated lognormal distribution using the maximum likelihood estimators that were extracted using the optim function follows very closely to the shape of the histogram of the data simulated from a left truncated lognormal distribution with $\mu=0$, $\sigma^2=1$, and truncation point $b=10$.

***

#### 3. (20pts) Simulate the data (n=1000) from a lognormal distribution with the parameters of your choice. Perform the following tasks:

```{r}
# Simulate Data
set.seed(123)
n <- 1000
mu <- 0
s2 <- 1
x <- rlnorm(n, meanlog = mu, sdlog = sqrt(s2))
```

##### a) Plot the histogram of your data in R

```{r}
hist(x, freq=FALSE, breaks=30, main=expression("Histogram of Lognormal Data with "~mu==0~" and "~sigma^2==1))
```

##### b) Estimate the LN-parameters using the Method of Moments. Show your derivations.

$$\mu_1'=\mu=E(Y)=e^{\mu+\frac{\sigma^2}{2}}$$

$$m_1'=\frac{1}{n}\sum_{i=0}^ny_i=\bar{Y}$$

$$\Rightarrow{e}^{\mu+\frac{\sigma^2}{2}}=\bar{Y}$$

$$\begin{aligned}
\mu_2'&=E(Y^2)=V(Y)+E(Y)^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{\mu+\frac{\sigma^2}{2}})^2\\
&=(e^{\sigma^2}-1)(e^{2\mu+\sigma^2})+(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}(e^{2\mu+\sigma^2})\\
&=e^{\sigma^2}\bar{Y}^2
\end{aligned}$$

$$m_2'=\frac{1}{n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{e}^{\sigma^2}\bar{Y}^2=\frac{1}{n}\sum_{i=0}^ny_i^2$$

Solve for $\hat{\mu}_{MM}$ and $\hat{\sigma}^2_{MM}$:

$$e^{\sigma^2}=\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2$$

$$\Rightarrow{\hat{\sigma}^2_{MM}}=\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)$$

$$\mu+\frac{\sigma^2}{2}=\ln(\bar{Y})$$

$$\begin{aligned}
\Rightarrow\hat{\mu}_{MM}&=\ln(\bar{Y})-\frac{\sigma^2}{2}\\
&=\ln(\bar{Y})-\frac{1}{2}\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)\\
&=\ln(\bar{Y})+\ln\left(\frac{1}{\bar{Y}^2n}\sum_{i=0}^ny_i^2\right)^{-\frac{1}{2}}\\
&=\ln(\bar{Y})+\ln\left(\bar{Y}\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)\\
&=\ln\left(\bar{Y}^2\sqrt{\frac{n}{\sum_{i=0}^ny_i^2}}\right)
\end{aligned}$$

```{r}
mmests = matrix(NA, 20000, 2)
for (i in 1:20000) {
  y <- rlnorm(1000, meanlog = 0, sdlog = 1)
  mu.mm <- log(((mean(y)^2))*sqrt(1000/sum(y^2)))
  s2.mm <- log((1/(mean(y)^2))*(1/1000)*sum(y^2))
  mmests[i,1] = mu.mm
  mmests[i,2] = s2.mm
}
mu.s2.mm <- colMeans(mmests)
mu.s2.mm
```

The method of moments estimates for a lognormal distribution with $\mu=0$ and $\sigma^2=1$ are:

$\hat{\mu}_{MM}=0.008678145$

$\hat{\sigma}^2_{MM}=0.981518599$

##### c) Estimate the LN-parameters using the Maximum Likelihood Estimation. Show your derivations.

$$\begin{aligned}
L(\mu,\sigma^2)&=f(y_1, y_2,...,y_n|\mu,\sigma^2)\\
&=\prod_{i=1}^n\frac{1}{y_i\sigma\sqrt{2\pi}}e^{-\frac{(\ln(y_i)-\mu)^2}{2\sigma^2}}\\
&=\left(\prod_{i=1}^n\frac{1}{y_i}\right)\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}}e^{-\frac{\sum_{i=1}^n(\ln(y_i)-\mu)^2}{2\sigma^2}}
\end{aligned}$$
$$\log(L(\mu,\sigma^2))=-\log(\prod_{i=1}^ny_i)-\frac{n}{2}\log(\sigma^2)-\frac{n}{2}\log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2$$
$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\mu}=\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)=0$$
$$\begin{aligned}
&\Rightarrow\sum_{i=1}^n\ln(y_i)-n\mu=0\\
&\Rightarrow{n}\mu=\sum_{i=1}^n\ln(y_i)\\
&\Rightarrow\hat{\mu}_{MLE}=\frac{1}{n}\sum_{i=1}^n\ln(y_i)
\end{aligned}$$

$$\frac{\partial{\log(L(\mu,\sigma^2))}}{\partial\sigma^2}=-\frac{n}{\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0$$
$$\begin{aligned}
&\Rightarrow{-n}+\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=0\\
&\Rightarrow\frac{1}{\sigma^2}\sum_{i=1}^n(\ln(y_i)-\mu)^2=n\\
&\Rightarrow\hat{\sigma}^2_{MLE}=\frac{1}{n}\sum_{i=1}^n(\ln(y_i)-\mu)^2\\
\end{aligned}$$

```{r}
mlests = matrix(NA, 20000, 2)
for (i in 1:20000) {
  y <- rlnorm(1000, meanlog = 0, sdlog = 1)
  mu.mle <- (1/n)*sum(log(y))
  s2.mle <- (1/n)*sum((log(y)-mu.mle)^2)
  mlests[i,1] = mu.mle
  mlests[i,2] = s2.mle
}
mu.s2.mle <- colMeans(mlests)
mu.s2.mle
```

The maximum likelihood estimates for a lognormal distribution with $\mu=0$ and $\sigma^2=1$ are:

$\hat{\mu}_{MLE}=-0.001214673$

$\hat{\sigma}^2_{MLE}=0.9994245861$

##### d) Plot the density using estimates in b) repeat the same using estimated in c). Overlay these two densities on your original histogram in a). Discuss your findings.

```{r}
hist(x, freq=FALSE, breaks = 30, ylim=c(0, 0.7), main="Plot of Density Using MM Estimates Over Lognormal Data")
a <- seq(0, 25, by = 0.01)
b <- dlnorm(a, mu.s2.mm[1], mu.s2.mm[2])
points(a, b, col="red")

hist(x, freq=FALSE, breaks = 30, ylim=c(0, 0.7), main="Plot of Density Using MLEs Over Lognormal Data")
c <- dlnorm(a, mu.s2.mle[1], mu.s2.mle[2])
points(a, c, col="red")
```

The shape of the plotted density of a lognormal distribution using the method of moments estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with $\mu=0$ and $\sigma^2=1$. The shape of the plotted density of a lognormal distribution using the maximum likelihood estimators follows very closely to the shape of the histogram of the data simulated from a lognormal distribution with $\mu=0$ and $\sigma^2=1$. From these graphs we cannot see an obvious difference in the performance of the estimators, so further comparisons of the estimators will be used to determine if the method of moments estimators or maximum likelihood estimators perform better.

##### e) Calculate the Bias and MSE for both estimators computed in b) and c). Present your summary of the results in a table format. Discuss your findings.

```{r}
# Calculate Bias
bias.mu.mm <- mu.s2.mm[1] - mu
bias.mu.mle <- mu.s2.mle[1] - mu
bias.s2.mm <- mu.s2.mm[2] - s2
bias.s2.mle <- mu.s2.mle[2] - s2

# Variance of the Estimators
var.mu.mm <- sd(mmests[,1])^2
var.mu.mle <- sd(mlests[,1])^2
var.s2.mm <- sd(mmests[,2])^2
var.s2.mle <- sd(mlests[,2])^2

# Calculate MSE
mse.mu.mm <- var.mu.mm + bias.mu.mm^2
mse.mu.mle <- var.mu.mle + bias.mu.mle^2
mse.s2.mm <- var.s2.mm + bias.s2.mm^2
mse.s2.mle <- var.s2.mle + bias.s2.mle^2

table <-  data.frame(bias.mu=c(bias.mu.mm, bias.mu.mle), bias.s2=c(bias.s2.mm, bias.s2.mle), mse.mu=c(mse.mu.mm, mse.mu.mle), mse.s2=c(mse.s2.mm, mse.s2.mle))
colnames(table) = c("Bias of $\\mu$", "Bias of $\\sigma^2$", "MSE of $\\mu$", "MSE of $\\sigma^2$")
rownames(table) = c("Method of Moments", "Maximum Likelihood Estimation")
kable(table)
```

The bias of $\hat{\mu}_{MM}$, $\hat{\mu}_{MLE}$, $\hat{\sigma}^2_{MM}$, and $\hat{\sigma}^2_{MLE}$ are all very close to 0, so the method of moments and maximum likelihood estimates appear to be good estimates for the lognormal parameters. The bias of $\hat{\mu}_{MLE}$ is less than the bias of $\hat{\mu}_{MM}$ for $\mu$ and the bias of $\hat{\sigma}^2_{MLE}$ is less than the bias of $\hat{\sigma}^2_{MM}$ for $\sigma^2$, so the maximum likelihood estimators seem to be better unbiased estimators for the lognormal parameters than the method of moments estimators. The MSE of $\hat{\mu}_{MM}$, $\hat{\mu}_{MLE}$, $\hat{\sigma}^2_{MM}$, and $\hat{\sigma}^2_{MLE}$ are also all very close to 0, so so the method of moments and maximum likelihood estimates perform well for estimating the lognormal parameters. The MSE of $\hat{\mu}_{MLE}$ is less than the MSE of $\hat{\mu}_{MM}$ and the MSE of $\hat{\sigma}^2_{MLE}$ is less than the MSE of $\hat{\sigma}^2_{MM}$, so the maximum likelihood estimators seem to perform better for estimating lognormal parameters than the method of moments estimators.

##### f) Calculate the efficiency of the estimator in c) relative to that in b) for both parameters of interest. Discuss your findings.

```{r}
eff.mu <- var.mu.mm/var.mu.mle
eff.mu
eff.s2 <- var.s2.mm/var.s2.mle
eff.s2
```

$eff(\hat{\mu}_{MLE},\hat{\mu}_{MM})=\frac{Var(\hat{\mu}_{MM})}{Var(\hat{\mu}_{MLE})}=3.858594>1$, so $\hat{\mu}_{MLE}$ is a better unbiased estimator for $\mu$ than $\hat{\mu}_{MM}$.

$eff(\hat{\sigma}^2_{MLE},\hat{\sigma}^2_{MM})=\frac{Var(\hat{\sigma}^2_{MM})}{Var(\hat{\sigma}^2_{MLE})}=9.191056>1$, so $\hat{\sigma}^2_{MLE}$ is a better unbiased estimator for $\sigma^2$ than $\hat{\sigma}^2_{MM}$.

Overall, the maximum likelihood estimators perform better than the method of moments estimators for estimating $\mu$ and $\sigma^2$ of a lognormal distribution. From the the likelihood function in part c, we find that $\sum_{i=1}^{n}ln(y_i)$ and $\sum_{i=1}^{n}ln(y_i)^2$ are both sufficient statistics for $\mu$ and $\sigma^2$. The MLEs found for $\mu$ and $\sigma^2$ found in part c are both functions of these sufficient statistics, while the method of moments estimators in part b are not. This means that $\hat{\mu}_{MLE}$ and $\hat{\sigma}^2_{MLE}$ are minimum varaince unbiased estimators, so it would be expected that they perform better at estimating $\mu$ and $\sigma^2$ than $\hat{\mu}_{MM}$ and $\hat{\sigma}^2_{MM}$.

***

#### 4. (20pts) Suppose you want to test the following hypotheses: 
Ho: μ = 30,000
Ha: μ  ≠ 30,000
Use α=0.05 and α=0.01.
##### a) Define a rejection region for a given α-level. Assume σ=5,000. All derivations must be included in the report. [Note: the sample size is small!]

Although the sample size is small, because we know the population standard deviation, we can use standard normal.

RR = {$\hat{\theta}$ < $\theta_0$ - $z_\alpha\sigma_\hat{\theta}$ or $\hat{\theta}$ > $\theta_0$ + $z_\alpha\sigma_\hat{\theta}$}

Therefore, RR = {$\hat\mu$ < $30000$ - $\frac{5000}{\sqrt{29}}z_.975$ or $\hat\mu$ > $30000$ + $\frac{5000}{\sqrt{29}}z_.975$}

```{r}
#RR for alpha = .05
30000 + (5000/sqrt(29))*qnorm(.025)
30000 + (5000/sqrt(29))*qnorm(.975)

#RR for alpha = .01
30000 + (5000/sqrt(29))*qnorm(.005)
30000 + (5000/sqrt(29))*qnorm(.995)
```

Given a small sample where n = 29:

For α=0.05, RR = {T < 28180.22 or T > 31819.78}

For α=0.01, RR = {T < 27608.4 or T > 32391.6}


##### b) Construct two Power Curves for various values of β- Type II error rate. These curves must be on the same plot. [Note: you will need to consider various values for the sample mean.] 

```{r}
powerfunction <- function(mu, sd, alpha1, alpha2, n) {
  vec <- seq(20001,39999,1)
  se <- (sd/sqrt(n))
  
  q1 = qnorm(1-alpha1/2)
  q2 = qnorm(1-alpha2/2)
  
  rr_lower1 <- ((mu-q1*se)-vec)/se
  rr_upper1 <- ((mu+q1*se)-vec)/se
  
  rr_lower2 <- ((mu-q2*se)-vec)/se
  rr_upper2 <- ((mu+q2*se)-vec)/se
  
  power1 <- pnorm(rr_upper1, lower.tail = FALSE) + 
    pnorm(rr_lower1, lower.tail=TRUE)
  power2 <- pnorm(rr_upper2, lower.tail = FALSE) +
    pnorm(rr_lower2, lower.tail=TRUE)
  
  
  ggplot() +
  geom_path(aes(x=vec, y=power1, color=".05"), size=0.8) +
  geom_path(aes(x=vec, y=power2, color=".01"), size=0.8) +
  labs(y="Power", x="mu", color="alpha")
  
}

powerfunction(30000,5000,.05,.01,29)
```

##### c) Reflect on your findings. How does α-level impact the shape of the power curve? How does power changes with α? How does power changes with β? A minimum 5 full sentences are required for full credit. 

Increasing α causes the shape of the power curve to becomes more narrow. The area under the curve also increases as α does. This is a result of power increasing as α increases. As the value of beta decreases, the power increases. This happens because the distance between the specified null value and the specified alternative value increases.

***

#### 5. (20pts) Provide a reflection about this project work by considering the following questions: 1) How does this project enhanced your learning about the course topics? How much this project sharpens your critical thinking skills? What were the difficulties that you were facing as you worked through this project? How did you resolve them? How much did you grow by working on some advanced topics in this course? How do you think this project would prepare you for your future job? What skills did you build working on the STA462 projects? The UAs are expected to work with you on this project. State specifically the name of the UA you worked with and how much help you got from your UA. I expect to see a min of 10 full sentences for this reflection.

This project enhanced our learning skills by having us apply techniques learned in class to unique cases of distributions familiar to us. Our critical thinking skills were enhanced through the challenges we faced. For example, we had to research the truncated distributions, find out their pdfs, and understand their equations before being able to tackle questions 1 and 2. Next, we had to apply the concepts of MLEs to these distributions by finding the MLEs by using the optim function. We also had to derive the formulas for the method of moments esimators and MLEs of the lognormal distribution and compare the performance of these estimators. Then, we had to create a function that produces a power curve for a two tailed normal distribution. We believe we learned a lot about tackling these tough problems, specifically growing in our ability to handle topics we've discussed, but haven't had a significant amount of experience working with them. The biggest difficulties we faced were deriving the CDFs for truncated normal and truncated log normal, and using MLE to estimate the log normal parameters. We resolved these problems by working together as a group and asking our UA Justus for help when we got stuck. I think this project prepared us for a future job by teaching us a lot about working as a group on a complex topic. We worked together to research and understand different things, and we learned to ask for help when we needed to. We also improved our skills of using R for various techniques, such as using the optim function to extract MLEs, generating samples to develop method of moments estimators and MLEs, or constructing power curves. Our UA Justus helped a lot on this project on a few occassions. We specifically reached out to him to check our parameters in question 3, help with the power curve in question 4, and to help understand how to fix our question 1. He did a great job of checking over are work when needed, and pushing us in the right direction whenever we got stuck.

##### Group Contributions:

Questions 1 and 2 were worked on as a group

Rachel: Q3

Ryan: Q4

Sean: Q5
